= Core Concepts
:compat-mode!:
// Metadata:
:description: Job Service to control timeouts in {product_name}
:keywords: sonataflow, workflow, serverless, timeout, timer, expiration, job service
// links
:jobs_service_image_url: https://quay.io/repository/kiegroup/kogito-jobs-service-allinone
:jobs_service_image_usage_url: https://github.com/kiegroup/kogito-images#jobs-services-all-in-one
:knative_eventing_url: https://knative.dev/docs/eventing/
:knative_eventing_trigger_url: https://knative.dev/docs/eventing/triggers/
:knative_eventing_sink_binding_url: https://knative.dev/docs/eventing/sinks/#sink-parameter-example

The Job Service is the {product_name}'s architecture supporting service, responsible for controlling the execution of time-triggered actions, also known as jobs.

A job execution is a callback to the {product_name} runtime application, and can be configured in different ways as described in the <<job-service-communication, communication>> section.

All time-base states that you can use in a workflow, are handled by the interaction between the job-service-addon, that you have configured in your project, and the Jobs Service.

For example, every time the workflow execution reaches a state with a configured timeout, a corresponding job is created at the Jobs Service, and when the timeout is met, the callback is executed to notify the workflow.


image::job-services/Time-Based-States-And-Job-Service-Interaction.png[]


[NOTE]
====
If the application is not configured to use the Job Service, all time-based actions will use an in-memory implementation of that service.
However, this setup must not be used in production, since every time the application is restarted, all the timers are lost.
This last is not suited for a serverless architectures, where the applications might scale to zero at any time.
====

Since the main goal of the Job Service is to work only with active jobs, such as the scheduled jobs that need to be executed, when a job reaches a final state, it is removed from the Job Service.
However, in cases where you want to keep historical information about the jobs, you can configure the Job Service to produce status change events, that can be collected by the {product_name} `Data
Index Service`, where they can be indexed and made available by GraphQL queries.

== Executing

To execute the Job Service in your docker or kubernetes environment, you must use the following link:{jobs_service_image_url}[image] available in the https://quay.io repository.
In the next topics you can see the different configuration parameters that you must use, for example to configure the persistence mechanism, the eventing system, etc.

[NOTE]
====
The Job Service link:{jobs_service_image_url}[image] is shipped with the PostgreSQL, Ephemeral (InMemory), and Infinispan persistence mechanisms that can be switched using the `JOBS_SERVICE_PERSISTENCE` configuration value. If not set, it defaults to the `ephemeral` option. For more information about the Job Service, the container image can be found link:{jobs_service_image_usage_url}[here].
====

== Persistence

An important configuration aspect of the Job Service is the persistence mechanism, it is where all the jobs information is stored, and guarantees no information is lost upon service restarts.

[#job-service-postgresql]
=== PostgreSQL

PostgreSQL is the recommended database to use with the Job Service.
Additionally, it provides an initialization procedure that integrates Flyway for the database initialization. Which automatically controls the database schema, in this way, the tables are created or updated by the service when required.

In case you need to externally control the database schema, you can check and apply the DDL scripts for the Job Service in the same way as described in
xref:persistence/postgresql-flyway-migration.adoc#manually-executing-scripts[Manually executing scripts] guide.


.List of parameters for the PostgreSQL configuration
[cols="2,1,1"]
|===
|Variable | Description| Example value

|JOBS_SERVICE_PERSISTENCE
|Configure the persistence mechanism that must be used.
|postgresql

|QUARKUS_DATASOURCE_USERNAME
|Username to connect to the database.
|postgres

|QUARKUS_DATASOURCE_PASSWORD
|Password to connect to the database
|pass

|QUARKUS_DATASOURCE_JDBC_URL
| JDBC datasource url used by Flyway to connect to the database.
|jdbc:postgresql://timeouts-showcase-database:5432/postgres?currentSchema=jobs-service

|QUARKUS_DATASOURCE_REACTIVE_URL
| Reactive datasource url used by the Job Service to connect to the database.
|postgresql://timeouts-showcase-database:5432/postgres?search_path=jobs-service

|===

The timeouts showcase example xref:use-cases/timeout-showcase-example.adoc#execute-quarkus-project-standalone-services[Quarkus Workflow Project with standalone services], shows how to run a PostgreSQL based Job Service as a Kubernetes deployment.
In your local environment you might have to change some of these values to point to your own PostgreSQL database.

=== Ephemeral
Alternatively, there is an in-memory database support that does not require any external database configuration. It can be used for testing and development purposes, but it is not recommended for production, since all jobs are lost upon a service restart or failure.

=== Infinispan

Complete the configuration parameters

== Eventing API

In addition to the REST API, that is always available, the Job Service provides a Cloud Event based API that can be used to create and delete jobs.
This API is useful in deployment scenarios where you want to use an event based communication from the workflows runtime to the Job Service. For the transport of these events you can use the <<knative-eventing, knative eventing>> system or the <<kafka-messaging, kafka messaging>> system.

[#knative-eventing]
=== Knative eventing

By default, the Job Service Eventing API, is prepared to work in a link:{knative_eventing_url}[knative eventing] system. This means that by adding no additional configurations parameters, it'll be able to receive cloud events via the link:{knative_eventing_url}[knative eventing] system to manage the jobs.
However, you must still configure your link:{knative_eventing_url}[knative eventing] environment to ensure these events are properly delivered to the Job Service, see <<knative-eventing-supporting-resources, knative eventing supporting resources>>.

Finally, the only configuration parameter that you must set, when needed, is to enable the propagation of the Job Status Change events, for example, if you want to register these events in the Data Index Service.

.List of parameters for the Knative eventing configuration
[cols="2,1,1"]
|===
|Variable | Description| Default value

|KOGITO_JOBS_SERVICE_KNATIVE_EVENTS
| true to establish if the Job Status Change events must be propagated.
| false

|===

[#knative-eventing-supporting-resources]
==== Knative eventing supporting resources

To ensure the Job Service receives the knative events to manage the jobs, you must configure the two link:{knative_eventing_trigger_url}[triggers] shown in the diagram below. Additionally, if you have enabled the Job Status Change events propagation you must create the link:{knative_eventing_sink_binding_url}[sink binding].

.Knative eventing supporting resources
image::job-services/Knative-Eventing-API-Resources.png[]

The following snippets shows an example on how you can configure these resources. Consider that these configurations might need to be adjusted in you local kubernetes cluster.
Additionally, you can see this example xref:use-cases/timeout-showcase-example.adoc#execute-quarkus-project-standalone-services[Quarkus Workflow Project with standalone services].

.Job Service trigger configuration example
[source,yaml]
----
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: jobs-service-postgresql-create-job-trigger
spec:
  broker: default
  filter:
    attributes:
      type: job.create
  subscriber:
    ref:
      apiVersion: v1
      kind: Service
      name: jobs-service-postgresql
    uri: /v2/jobs/events
----

.Job Service sink binding configuration example
[source,yaml]
----
apiVersion: sources.knative.dev/v1
kind: SinkBinding
metadata:
  name: jobs-service-postgresql-sb
spec:
  sink:
    ref:
      apiVersion: eventing.knative.dev/v1
      kind: Broker
      name: default
  subject:
    apiVersion: apps/v1
    kind: Deployment
    selector:
      matchLabels:
        app.kubernetes.io/name: jobs-service-postgresql
        app.kubernetes.io/version: 2.0.0-SNAPSHOT
----

[#kafka-messaging]
=== Kafka messaging

To configure the Jobs Service to work with the kafka messaging system you must configure the following parameters.

.List of parameters for the kafka messaging configuration
[cols="2,1,1"]
|===
|Variable | Description| Example value

|QUARKUS_PROFILE
|Set the value `events-support` to enable the integration with the kafka messaging system.
|events-support

|KAFKA_BOOTSTRAP_SERVERS
|A comma-separated list of host:port pairs identifying the Kafka bootstrap server(s).
|localhost:9092

|===

== Leader election

Currently, the Job Service works in a single instance manner where there should be just one active instance of the service.

To avoid issues when deploying the service in the cloud, where it is common to eventually have more than one instance deployed, the Job Service supports a leader instance election process. Only the instance that becomes the leader activates the external communication to receive and schedule jobs.

In all instances who are not leaders, stay inactive in a kind of wait state and try to become the leader continuously.

When a new instance of the service is started, it is not set as a leader at startup time but instead, it starts the process to become one.

When an instance that is the leader for any issue stays unresponsive or is shut down, one of the other running instances becomes the leader.

.Job Service leader election
image::job-services/job-service-leader.png[]

[NOTE]
====
This leader election mechanism uses the underlying persistence backend, which currently is only supported in the PostgreSQL implementation.
====

There is no need for any configuration to support this feature, the only requirement is to have the supported database with the data schema up-to-date as described in the <<job-service-postgresql>> section.

In case the underlying persistence does not support this feature, you must guarantee that just one single instance of the Job Service is running at the same time.
that just one single instance of the Job Service is running at the same time.

[#job-service-communication]
== Job Service communication

[NOTE]
====
The Job Service does not execute a job but triggers a callback that might be an HTTP request or a Cloud Event that is
managed by the configured xref:core/timeouts-support.adoc#job-addon-configuration[jobs addon] in the workflow application.
====

=== Knative Eventing

To configure the communication between the Job Service and the workflow runtime through the Knative eventing system, you must provide a set of configurations.

The Job Service configuration is accomplished through the deployment descriptor shown in the xref:use-cases/timeout-showcase-example.adoc#job-service-deploy[example].


== Additional resources

* xref:core/timeouts-support.adoc[Timeouts in {product_name}]
* xref:use-cases/timeout-showcase-example.adoc[Timeout example in {product_name}]

include::../../pages/_common-content/report-issue.adoc[]